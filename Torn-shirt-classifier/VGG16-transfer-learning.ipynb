{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Transfer Learning in PyTorch.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "toc_visible": true
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.6"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YSr7nMRd2f_3",
        "colab_type": "text"
      },
      "source": [
        " ## Dataset: Torn Clothes\n",
        "\n",
        "The dataset is comprised of clothes where some of them are torn other not. <br>So we have 2 categories. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9rvWCuNa2f_5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# PyTorch\n",
        "from torchvision import transforms, datasets, models\n",
        "import torch\n",
        "from torch import optim, cuda\n",
        "from torch.utils.data import DataLoader, sampler\n",
        "import torch.nn as nn\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore', category=FutureWarning)\n",
        "\n",
        "# Data science tools\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import os\n",
        "\n",
        "# Image manipulations\n",
        "from PIL import Image\n",
        "# Useful for examining network\n",
        "from torchsummary import summary\n",
        "# Timing utility\n",
        "from timeit import default_timer as timer\n",
        "\n",
        "# Visualizations\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "%matplotlib inline\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jIPst-aI2f__",
        "colab_type": "text"
      },
      "source": [
        "### Parameters\n",
        "\n",
        "The parameters in this cell can be changed as needed. Since this is small dataset hence I'm going to be train it with single gpu, but your setup will probably be different. <p>One of the benefits of using PyTorch is we can easily move different elements of the model or data to cpus/gpus or to multiple gpus. Granted, we probably won't always have many gpus, but when we do, we want to make the best use of them! "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A5w69LdK2gAA",
        "colab_type": "code",
        "outputId": "059ebd01-1119-4fc0-cd1f-2cb8502f81eb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        }
      },
      "source": [
        "\n",
        "# Location of data\n",
        "datadir = 'all/'\n",
        "traindir =datadir+'train/'\n",
        "validdir =datadir+'valid/'\n",
        "testdir =datadir+'test/'\n",
        "\n",
        "save_file_name = 'vgg16-transfer-4.pt'\n",
        "checkpoint_path = 'vgg16-transfer-4.pth'\n",
        "\n",
        "# Change to fit hardware\n",
        "batch_size = 1\n",
        "\n",
        "# Whether to train on a gpu\n",
        "train_on_gpu = cuda.is_available()\n",
        "print(f'Train on gpu: {train_on_gpu}')\n",
        "\n",
        "#If you use multiple gpus it turns statement multi_gpu = True. Probably useful for large datasets\n",
        "# Number of gpus\n",
        "if train_on_gpu:\n",
        "    gpu_count = cuda.device_count()\n",
        "    print(f'{gpu_count} gpus detected.')\n",
        "    if gpu_count > 1:\n",
        "        multi_gpu = True\n",
        "    else:\n",
        "        multi_gpu = False"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on gpu: True\n",
            "1 gpus detected.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NPzq2I6u6Hue",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#command to insert files/daatset in colab\n",
        "#from zipfile import ZipFile\n",
        "#file_name = \"all.zip\"\n",
        "\n",
        "#with ZipFile(file_name,'r') as zip:\n",
        "#  zip.extractall()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "isTKa_8m-rZE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Command to remove files from colab\n",
        "#rm -rf all.zip"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FDhPe0Ho2gAM",
        "colab_type": "text"
      },
      "source": [
        "Below we take a look at the number of images in each category and the size of the images."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4A6pZwY02gAN",
        "colab_type": "code",
        "outputId": "c881d33a-4313-4153-ab59-9a5a35f2b016",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 106
        }
      },
      "source": [
        "# Empty lists\n",
        "categories = []\n",
        "img_categories = []\n",
        "n_train = []\n",
        "n_valid = []\n",
        "n_test = []\n",
        "hs = []\n",
        "ws = []\n",
        "\n",
        "# Iterate through each category\n",
        "for d in os.listdir(traindir):\n",
        "    categories.append(d)\n",
        "\n",
        "    # Number of each image\n",
        "    train_imgs = os.listdir(traindir + d)\n",
        "    valid_imgs = os.listdir(validdir + d)\n",
        "    test_imgs = os.listdir(testdir + d)\n",
        "    n_train.append(len(train_imgs))\n",
        "    n_valid.append(len(valid_imgs))\n",
        "    n_test.append(len(test_imgs))\n",
        "\n",
        "    # Find stats for train images\n",
        "    for i in train_imgs:\n",
        "        img_categories.append(d)\n",
        "        img = Image.open(traindir + d + '/' + i)\n",
        "        img_array = np.array(img)\n",
        "        # Shape\n",
        "        hs.append(img_array.shape[0])\n",
        "        ws.append(img_array.shape[1])\n",
        "\n",
        "# Dataframe of categories\n",
        "cat_df = pd.DataFrame({'category': categories,\n",
        "                       'n_train': n_train,\n",
        "                       'n_valid': n_valid, 'n_test': n_test}).\\\n",
        "    sort_values('category')\n",
        "\n",
        "# Dataframe of training images\n",
        "image_df = pd.DataFrame({\n",
        "    'category': img_categories,\n",
        "    'height': hs,\n",
        "    'width': ws\n",
        "})\n",
        "\n",
        "cat_df.sort_values('n_train', ascending=False, inplace=True)\n",
        "cat_df.head()"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>category</th>\n",
              "      <th>n_train</th>\n",
              "      <th>n_valid</th>\n",
              "      <th>n_test</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>not</td>\n",
              "      <td>14</td>\n",
              "      <td>3</td>\n",
              "      <td>4</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>sure</td>\n",
              "      <td>13</td>\n",
              "      <td>3</td>\n",
              "      <td>4</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "  category  n_train  n_valid  n_test\n",
              "1      not       14        3       4\n",
              "0     sure       13        3       4"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oF6trI1i2gAT",
        "colab_type": "text"
      },
      "source": [
        "#### Distribution of Images\n",
        "\n",
        "There are  13 and 14 training images in each sure and not sure category. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tCBE1B1Z2gAh",
        "colab_type": "text"
      },
      "source": [
        "#### Distribution of Images Sizes\n",
        "\n",
        "The images themselves have vastly different shapes. We can see this by looking at the stats of images sizes by category."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qle3cuVA2gAi",
        "colab_type": "code",
        "outputId": "ffeba341-a3ed-471a-f800-41ab28833db2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 166
        }
      },
      "source": [
        "img_dsc = image_df.groupby('category').describe()\n",
        "img_dsc.head()"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead tr th {\n",
              "        text-align: left;\n",
              "    }\n",
              "\n",
              "    .dataframe thead tr:last-of-type th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr>\n",
              "      <th></th>\n",
              "      <th colspan=\"8\" halign=\"left\">height</th>\n",
              "      <th colspan=\"8\" halign=\"left\">width</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th></th>\n",
              "      <th>count</th>\n",
              "      <th>mean</th>\n",
              "      <th>std</th>\n",
              "      <th>min</th>\n",
              "      <th>25%</th>\n",
              "      <th>50%</th>\n",
              "      <th>75%</th>\n",
              "      <th>max</th>\n",
              "      <th>count</th>\n",
              "      <th>mean</th>\n",
              "      <th>std</th>\n",
              "      <th>min</th>\n",
              "      <th>25%</th>\n",
              "      <th>50%</th>\n",
              "      <th>75%</th>\n",
              "      <th>max</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>category</th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>not</th>\n",
              "      <td>14.0</td>\n",
              "      <td>228.571429</td>\n",
              "      <td>22.283883</td>\n",
              "      <td>178.0</td>\n",
              "      <td>225.0</td>\n",
              "      <td>225.0</td>\n",
              "      <td>246.5</td>\n",
              "      <td>259.0</td>\n",
              "      <td>14.0</td>\n",
              "      <td>218.571429</td>\n",
              "      <td>17.500392</td>\n",
              "      <td>194.0</td>\n",
              "      <td>204.5</td>\n",
              "      <td>222.5</td>\n",
              "      <td>225.0</td>\n",
              "      <td>266.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>sure</th>\n",
              "      <td>13.0</td>\n",
              "      <td>229.615385</td>\n",
              "      <td>33.949812</td>\n",
              "      <td>183.0</td>\n",
              "      <td>198.0</td>\n",
              "      <td>225.0</td>\n",
              "      <td>258.0</td>\n",
              "      <td>275.0</td>\n",
              "      <td>13.0</td>\n",
              "      <td>223.461538</td>\n",
              "      <td>34.798504</td>\n",
              "      <td>183.0</td>\n",
              "      <td>185.0</td>\n",
              "      <td>225.0</td>\n",
              "      <td>255.0</td>\n",
              "      <td>275.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "         height                                ...  width                     \n",
              "          count        mean        std    min  ...    25%    50%    75%    max\n",
              "category                                       ...                            \n",
              "not        14.0  228.571429  22.283883  178.0  ...  204.5  222.5  225.0  266.0\n",
              "sure       13.0  229.615385  33.949812  183.0  ...  185.0  225.0  255.0  275.0\n",
              "\n",
              "[2 rows x 16 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G5vLcG482gAt",
        "colab_type": "text"
      },
      "source": [
        "When we use the images in the pre-trained network, we'll have to reshape them to 224 x 224. This is the size of Imagenet images and is therefore what the model expects. The images that are larger than this will be truncated while the smaller images will be interpolated. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fFj4U0VO2gA5",
        "colab_type": "text"
      },
      "source": [
        "# Image Preprocessing\n",
        "\n",
        "To prepare the images for our network, we have to resize them to 64 x 64 and normalize each color channel by subtracting a mean value and dividing by a standard deviation. We will also augment our training data in this stage. These operations are done using image `transforms`, which prepare our data for a neural network.\n",
        "\n",
        "### Data Augmentation\n",
        "\n",
        "Because there are a limited number of images in some categories, we can use image augmentation to artificially increase the number of images \"seen\" by the network. This means for training, we randomly resize and crop the images and also flip them horizontally. A different random transformation is applied each epoch (while training), so the network effectively sees many different versions of the same image. All of the data is also converted to Torch `Tensor`s before normalization. The validation and testing data is not augmented but is only resized and normalized. The normalization values are standardized for Imagenet. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Os0mRmPj2gA6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Image transformations\n",
        "image_transforms = {\n",
        "    # Train uses data augmentation\n",
        "    'train':\n",
        "    transforms.Compose([\n",
        "        transforms.RandomResizedCrop(size=200, scale=(0.8, 1.0)),\n",
        "        transforms.RandomRotation(degrees=30),\n",
        "        transforms.ColorJitter(),\n",
        "        transforms.RandomHorizontalFlip(),\n",
        "        transforms.CenterCrop(size=64),  # Image net standards\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize([0.485, 0.456, 0.406],\n",
        "                             [0.229, 0.224, 0.225])  # Imagenet standards\n",
        "    ]),\n",
        "    # Validation does not use augmentation\n",
        "    'val':\n",
        "    transforms.Compose([\n",
        "        transforms.Resize(size=64),\n",
        "        transforms.CenterCrop(size=180),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
        "    ]),\n",
        "    # Test does not use augmentation\n",
        "    'test':\n",
        "    transforms.Compose([\n",
        "        transforms.Resize(size=64),\n",
        "        transforms.CenterCrop(size=180),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
        "    ]),\n",
        "}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wPluPJBk2gBE",
        "colab_type": "text"
      },
      "source": [
        "We'll work with two example images and apply the train transformations."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "71J0YzHu2gBT",
        "colab_type": "text"
      },
      "source": [
        "## Data Iterators\n",
        "\n",
        "To avoid loading all of the data into memory at once, we use training `DataLoaders`. First, we create a dataset object from the image folders, and then we pass these to a `DataLoader`. At training time, the `DataLoader` will load the images from disk, apply the transformations, and yield a batch. To train and validation, we'll iterate through all the batches in the respective `DataLoader`. \n",
        "\n",
        "One crucial aspect is to `shuffle` the data before passing it to the network. This means that the ordering of the image categories changes on each pass through the data (one pass through the data is one training epoch). "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ptwoWACO2gBU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Datasets from each folder\n",
        "data = {\n",
        "    'train':\n",
        "    datasets.ImageFolder(root=traindir, transform=image_transforms['train']),\n",
        "    'val':\n",
        "    datasets.ImageFolder(root=validdir, transform=image_transforms['val']),\n",
        "    'test':\n",
        "    datasets.ImageFolder(root=testdir, transform=image_transforms['test'])\n",
        "}\n",
        "\n",
        "# Dataloader iterators\n",
        "dataloaders = {\n",
        "    'train': DataLoader(data['train'], batch_size=batch_size, shuffle=True),\n",
        "    'val': DataLoader(data['val'], batch_size=batch_size, shuffle=True),\n",
        "    'test': DataLoader(data['test'], batch_size=batch_size, shuffle=True)\n",
        "}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rOUx0iGI2gBX",
        "colab_type": "code",
        "outputId": "6a6c5725-19d0-4eeb-c1bd-cce5f084f815",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "trainiter = iter(dataloaders['train'])\n",
        "features, labels = next(trainiter)\n",
        "features.shape, labels.shape"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(torch.Size([1, 3, 64, 64]), torch.Size([1]))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MNTzaID42gBa",
        "colab_type": "text"
      },
      "source": [
        "The shape of a batch is `(batch_size, color_channels, height, width)`. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZB6elypE2gBa",
        "colab_type": "text"
      },
      "source": [
        "There should be 2 different classes. We can confirm this as follows."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xho-_1KJ2gBb",
        "colab_type": "code",
        "outputId": "27419f0b-984b-420e-9bd3-d96fe9249056",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        }
      },
      "source": [
        "n_classes = len(cat_df)\n",
        "print(f'There are {n_classes} different classes.')\n",
        "\n",
        "len(data['train'].classes)"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "There are 2 different classes.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "2"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WjBiP5YV2gBf",
        "colab_type": "text"
      },
      "source": [
        "We can iterate through the `DataLoaders` when doing training, validation, and testing. This construction avoids the need to load all the data into memory and also will automatically apply the transformations to each batch. On each epoch, the `Random` transformations will be different so the network will essentially see multiple versions of each training image. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SLHO43zb2gBg",
        "colab_type": "text"
      },
      "source": [
        "# Pre-Trained Models for Image Classification\n",
        "\n",
        "PyTorch has many pretrained models we can use. All of these models have been trained on Imagenet which consists of millions of images across 1000 categories. What we want to do with pretrained models is freeze the early layers, and replace the classification module with our own. \n",
        "\n",
        "## Approach\n",
        "\n",
        "The approach for using a pre-trained image recognition model is well-established:\n",
        "\n",
        "1. Load in pre-trained weights from a network trained on a large dataset\n",
        "2. Freeze all the weights in the lower (convolutional) layers\n",
        "    * Layers to freeze can be adjusted depending on similarity of task to large training dataset\n",
        "3. Replace the classifier (fully connected) part of the network with a custom classifier\n",
        "    * Number of outputs must be set equal to the number of classes\n",
        "4. Train only the custom classifier (fully connected) layers for the task\n",
        "    * Optimizer model classifier for smaller dataset\n",
        "    \n",
        "The idea behind pre-training is the early convolutional layers of a cnn extract features that are relevant for many image recognition tasks. The later, fully-connected layers, specialize to the specific dataset by learning higher-level features. Therefore, we can use the already trained convolutional layers while training only the fully-connected layers on our own dataset. Pre-trained networks have proven to be reasonably successful for a variety of tasks, and result in a significant reduction in training time and usually increases in performance. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xcv5raaD2gBl",
        "colab_type": "text"
      },
      "source": [
        "We'll be using the `vgg16`. The VGG networks had very good performance without taking a long time to train. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9mdom7GC2gBl",
        "colab_type": "text"
      },
      "source": [
        "## Process to Use Pre-Trained Model\n",
        "\n",
        "We'll illustrate the process by using one model, vgg16.\n",
        "\n",
        "First off, load in the model with pretrained weights. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "neTcjDN82gBm",
        "colab_type": "code",
        "outputId": "5f42cf9d-62c3-461b-cf1a-ec6ecf18b6ec",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 806
        }
      },
      "source": [
        "model = models.vgg16(pretrained=True)\n",
        "model"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading: \"https://download.pytorch.org/models/vgg16-397923af.pth\" to /root/.cache/torch/checkpoints/vgg16-397923af.pth\n",
            "100%|██████████| 553433881/553433881 [00:03<00:00, 138711872.17it/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "VGG(\n",
              "  (features): Sequential(\n",
              "    (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (1): ReLU(inplace)\n",
              "    (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (3): ReLU(inplace)\n",
              "    (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
              "    (5): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (6): ReLU(inplace)\n",
              "    (7): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (8): ReLU(inplace)\n",
              "    (9): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
              "    (10): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (11): ReLU(inplace)\n",
              "    (12): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (13): ReLU(inplace)\n",
              "    (14): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (15): ReLU(inplace)\n",
              "    (16): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
              "    (17): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (18): ReLU(inplace)\n",
              "    (19): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (20): ReLU(inplace)\n",
              "    (21): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (22): ReLU(inplace)\n",
              "    (23): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
              "    (24): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (25): ReLU(inplace)\n",
              "    (26): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (27): ReLU(inplace)\n",
              "    (28): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (29): ReLU(inplace)\n",
              "    (30): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
              "  )\n",
              "  (avgpool): AdaptiveAvgPool2d(output_size=(7, 7))\n",
              "  (classifier): Sequential(\n",
              "    (0): Linear(in_features=25088, out_features=4096, bias=True)\n",
              "    (1): ReLU(inplace)\n",
              "    (2): Dropout(p=0.5)\n",
              "    (3): Linear(in_features=4096, out_features=4096, bias=True)\n",
              "    (4): ReLU(inplace)\n",
              "    (5): Dropout(p=0.5)\n",
              "    (6): Linear(in_features=4096, out_features=1000, bias=True)\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VXw-tc_R2gBq",
        "colab_type": "text"
      },
      "source": [
        "The `classifier` is the part of the model that we'll train. However, for the vgg, we'll only need to train the last few layers in the classifier and not even all of the fully connected layers. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yQMKPF-X2gBr",
        "colab_type": "text"
      },
      "source": [
        "### Freeze Early layers\n",
        "\n",
        "We freeze all of the existing layers in the network by setting `requires_grad` to `False`. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TJyEyExS2gBs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Freeze early layers\n",
        "for param in model.parameters():\n",
        "    param.requires_grad = False"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nELtvmmF2gBv",
        "colab_type": "text"
      },
      "source": [
        "### Add on Custom Classifier\n",
        "\n",
        "We'll train a classifier consisting of the following layers\n",
        "\n",
        "* Fully connected with ReLU activation (n_inputs, 256)\n",
        "* Dropout with 40% chance of dropping\n",
        "* Fully connected with log softmax output (256, n_classes)\n",
        "\n",
        "To build our custom classifier, we use the `nn.Sequential()` module which allows us to specify each layer one after the other. We assign our custom classifier to the final `classifier` layer in the already trained vgg network. When we add on the extra layers, they are set to `require_grad=True` by default. These will be the only layers that are trained. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7wptoH132gBw",
        "colab_type": "code",
        "outputId": "106d3135-a4a8-4518-e16d-83d1656d20ca",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 269
        }
      },
      "source": [
        "n_inputs = model.classifier[6].in_features\n",
        "\n",
        "# Add on classifier\n",
        "model.classifier[6] = nn.Sequential(\n",
        "    nn.Linear(n_inputs, 256), nn.ReLU(), nn.Dropout(0.4),\n",
        "    nn.Linear(256, n_classes), nn.LogSoftmax(dim=1))\n",
        "\n",
        "model.classifier"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Sequential(\n",
              "  (0): Linear(in_features=25088, out_features=4096, bias=True)\n",
              "  (1): ReLU(inplace)\n",
              "  (2): Dropout(p=0.5)\n",
              "  (3): Linear(in_features=4096, out_features=4096, bias=True)\n",
              "  (4): ReLU(inplace)\n",
              "  (5): Dropout(p=0.5)\n",
              "  (6): Sequential(\n",
              "    (0): Linear(in_features=4096, out_features=256, bias=True)\n",
              "    (1): ReLU()\n",
              "    (2): Dropout(p=0.4)\n",
              "    (3): Linear(in_features=256, out_features=2, bias=True)\n",
              "    (4): LogSoftmax()\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fdTkESZD2gB2",
        "colab_type": "text"
      },
      "source": [
        "The final output will be log probabilities which we can then use in the Negative Log Likelihood Loss. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hSezb7hG2gB3",
        "colab_type": "code",
        "outputId": "f8716235-7673-4d41-d2c5-2e11fd0b0929",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        }
      },
      "source": [
        "total_params = sum(p.numel() for p in model.parameters())\n",
        "print(f'{total_params:,} total parameters.')\n",
        "total_trainable_params = sum(\n",
        "    p.numel() for p in model.parameters() if p.requires_grad)\n",
        "print(f'{total_trainable_params:,} training parameters.')"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "135,309,890 total parameters.\n",
            "1,049,346 training parameters.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a5qO0rzY2gB7",
        "colab_type": "text"
      },
      "source": [
        "Even with only a few layers set to trainable, there are still over a million parameters (weights) that will be updated during training. In effect, we are _fine-tuning_ the model to work on our problem. We already know it works well on Imagenet, and because our images are relatively similar, we should expect the model to easily _transfer_ its knowledge from Imagenet to our dataset."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tjRFH4cR2gB7",
        "colab_type": "text"
      },
      "source": [
        "### Move to GPU \n",
        "\n",
        "To use a gpu in PyTorch, we simply move the whole model onto the gpu. Later we'll have to move the data to gpus in our training loop."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9sBNbZrE2gB8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "if train_on_gpu:\n",
        "    model = model.to('cuda')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8Lq6utW_2gB-",
        "colab_type": "text"
      },
      "source": [
        "## Function to Load in Pretrained Model\n",
        "\n",
        "We can refactor all that code into a single function that returns a pretrained model. This only accepts the vgg16 or resnet50 at the moment but can be extended to use other models. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RSwxPMgF2gB-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_pretrained_model(model_name):\n",
        "\n",
        "    if model_name == 'vgg16':\n",
        "        model = models.vgg16(pretrained=True)\n",
        "\n",
        "        # Freeze early layers\n",
        "        for param in model.parameters():\n",
        "            param.requires_grad = False\n",
        "        n_inputs = model.classifier[6].in_features\n",
        "\n",
        "        # Add on classifier\n",
        "        model.classifier[6] = nn.Sequential(\n",
        "            nn.Linear(n_inputs, 256), nn.ReLU(), nn.Dropout(0.2),\n",
        "            nn.Linear(256, n_classes), nn.LogSoftmax(dim=1))\n",
        "\n",
        "    # Move to gpu and parallelize\n",
        "    if train_on_gpu:\n",
        "        model = model.to('cuda')\n",
        "\n",
        "    return model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3F7kLG0b2gCB",
        "colab_type": "text"
      },
      "source": [
        "This should return the same as the pretrained model with the custom classifier. In the case of resnet, we replace the `fc` layers with the same classifier.\n",
        "\n",
        "The `torchsummary` library has a helpful function called `summary` which summarizes our model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ji25if-R2gCD",
        "colab_type": "code",
        "outputId": "0f3559fa-d990-40a9-b583-36ac07820ea9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 958
        }
      },
      "source": [
        "model = get_pretrained_model('vgg16')\n",
        "summary(model, input_size=(3, 224, 224), batch_size=batch_size, device='cuda')"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "----------------------------------------------------------------\n",
            "        Layer (type)               Output Shape         Param #\n",
            "================================================================\n",
            "            Conv2d-1          [1, 64, 224, 224]           1,792\n",
            "              ReLU-2          [1, 64, 224, 224]               0\n",
            "            Conv2d-3          [1, 64, 224, 224]          36,928\n",
            "              ReLU-4          [1, 64, 224, 224]               0\n",
            "         MaxPool2d-5          [1, 64, 112, 112]               0\n",
            "            Conv2d-6         [1, 128, 112, 112]          73,856\n",
            "              ReLU-7         [1, 128, 112, 112]               0\n",
            "            Conv2d-8         [1, 128, 112, 112]         147,584\n",
            "              ReLU-9         [1, 128, 112, 112]               0\n",
            "        MaxPool2d-10           [1, 128, 56, 56]               0\n",
            "           Conv2d-11           [1, 256, 56, 56]         295,168\n",
            "             ReLU-12           [1, 256, 56, 56]               0\n",
            "           Conv2d-13           [1, 256, 56, 56]         590,080\n",
            "             ReLU-14           [1, 256, 56, 56]               0\n",
            "           Conv2d-15           [1, 256, 56, 56]         590,080\n",
            "             ReLU-16           [1, 256, 56, 56]               0\n",
            "        MaxPool2d-17           [1, 256, 28, 28]               0\n",
            "           Conv2d-18           [1, 512, 28, 28]       1,180,160\n",
            "             ReLU-19           [1, 512, 28, 28]               0\n",
            "           Conv2d-20           [1, 512, 28, 28]       2,359,808\n",
            "             ReLU-21           [1, 512, 28, 28]               0\n",
            "           Conv2d-22           [1, 512, 28, 28]       2,359,808\n",
            "             ReLU-23           [1, 512, 28, 28]               0\n",
            "        MaxPool2d-24           [1, 512, 14, 14]               0\n",
            "           Conv2d-25           [1, 512, 14, 14]       2,359,808\n",
            "             ReLU-26           [1, 512, 14, 14]               0\n",
            "           Conv2d-27           [1, 512, 14, 14]       2,359,808\n",
            "             ReLU-28           [1, 512, 14, 14]               0\n",
            "           Conv2d-29           [1, 512, 14, 14]       2,359,808\n",
            "             ReLU-30           [1, 512, 14, 14]               0\n",
            "        MaxPool2d-31             [1, 512, 7, 7]               0\n",
            "AdaptiveAvgPool2d-32             [1, 512, 7, 7]               0\n",
            "           Linear-33                  [1, 4096]     102,764,544\n",
            "             ReLU-34                  [1, 4096]               0\n",
            "          Dropout-35                  [1, 4096]               0\n",
            "           Linear-36                  [1, 4096]      16,781,312\n",
            "             ReLU-37                  [1, 4096]               0\n",
            "          Dropout-38                  [1, 4096]               0\n",
            "           Linear-39                   [1, 256]       1,048,832\n",
            "             ReLU-40                   [1, 256]               0\n",
            "          Dropout-41                   [1, 256]               0\n",
            "           Linear-42                     [1, 2]             514\n",
            "       LogSoftmax-43                     [1, 2]               0\n",
            "================================================================\n",
            "Total params: 135,309,890\n",
            "Trainable params: 1,049,346\n",
            "Non-trainable params: 134,260,544\n",
            "----------------------------------------------------------------\n",
            "Input size (MB): 0.57\n",
            "Forward/backward pass size (MB): 218.78\n",
            "Params size (MB): 516.17\n",
            "Estimated Total Size (MB): 735.52\n",
            "----------------------------------------------------------------\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Tv56WhSA2gCJ",
        "colab_type": "text"
      },
      "source": [
        "We can see that the model is quite large and training all of the layers would take a considerable time. Even with only a few layers to train, this can still take a while to train. You might need to decrease the `batch_size` if this is not fitting on your gpu (hopefully you have one)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hXEXu9Jj2gCJ",
        "colab_type": "code",
        "outputId": "633d8d81-0474-45cc-ff55-1ce19f608c42",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 134
        }
      },
      "source": [
        "print(model.classifier[6])"
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Sequential(\n",
            "  (0): Linear(in_features=4096, out_features=256, bias=True)\n",
            "  (1): ReLU()\n",
            "  (2): Dropout(p=0.2)\n",
            "  (3): Linear(in_features=256, out_features=2, bias=True)\n",
            "  (4): LogSoftmax()\n",
            ")\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6fGzyESW2gCM",
        "colab_type": "text"
      },
      "source": [
        "#### Mapping of Classes to Indexes\n",
        "\n",
        "To keep track of the predictions made by the model, we create a mapping of classes to indexes and indexes to classes. This will let us know the actual class for a given prediction."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YfLDp8f12gCN",
        "colab_type": "code",
        "outputId": "0333536f-3b8e-43a1-d5f9-561af7d949f8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "model.class_to_idx = data['train'].class_to_idx\n",
        "model.idx_to_class = {\n",
        "    idx: class_\n",
        "    for class_, idx in model.class_to_idx.items()\n",
        "}\n",
        "\n",
        "list(model.idx_to_class.items())[:10]"
      ],
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[(0, 'not'), (1, 'sure')]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s-J6Q5Nj2gCP",
        "colab_type": "text"
      },
      "source": [
        "# Training Loss and Optimizer\n",
        "\n",
        "The loss is the negative log likelihood and the optimizer is the Adam optimizer. The negative log likelihood in PyTorch expects log probabilities so we need to pass it the raw output from the log softmax in our model's final layer. The optimizer is told to optimizer the model parameters (only a few of which require a gradient). \n",
        "\n",
        "* Loss (criterion): keeps track of the loss itself and the gradients of the loss with respect to the model parameters (weights)\n",
        "* Optimizer: updates the parameters (weights) with the gradients "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Kr41QQeJ2gCQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "criterion = nn.NLLLoss()\n",
        "optimizer = optim.Adam(model.parameters())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ESt9KKCl2gCT",
        "colab_type": "text"
      },
      "source": [
        "Below we can look at the parameters (weights) that will be updated by the optimizer during training."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xUS4-InO2gCT",
        "colab_type": "code",
        "outputId": "6b5fc806-76f3-4c3c-caf4-b5348b6f1048",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 84
        }
      },
      "source": [
        "for p in optimizer.param_groups[0]['params']:\n",
        "    if p.requires_grad:\n",
        "        print(p.shape)"
      ],
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "torch.Size([256, 4096])\n",
            "torch.Size([256])\n",
            "torch.Size([2, 256])\n",
            "torch.Size([2])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Src8GbMc2gCW",
        "colab_type": "text"
      },
      "source": [
        "# Training\n",
        "\n",
        "For training, we iterate through the train `DataLoader`, each time passing one batch through the model. One complete pass through the training data is known as an `epoch`, and we train for a set number of epochs or until early stopping kicks in (more below). After each batch, we calculate the loss (with `criterion(output, targets)`) and then calculate the gradients of the loss with respect to the model parameters with `loss.backward()`. This uses autodifferentiation and backpropagation to calculate the gradients. \n",
        "\n",
        "After calculating the gradients, we call `optimizer.step()` to update the model parameters with the gradients. This is done on every training batch so we are implementing stochastic gradient descent (or rather a version of it with momentum known as Adam). For each batch, we also compute the accuracy for monitoring and after the training loop has completed, we start the validation loop. This will be used to carry out early stopping.\n",
        "\n",
        "\n",
        "## Early Stopping\n",
        "\n",
        "Early stopping halts the training when the validation loss has not decreased for a number of epochs. Each time the validation loss does decrease, the model weights are saved so we can later load in the best model. Early stopping is an effective method to prevent overfitting on the training data. If we continue training, the training loss will continue to decrease, but the validation loss will increase because the model is starting to memorize the training data. Early stopping prevents this from happening, and, if we save the model each epoch when the validation loss decreases, we are able to retrieve the model that does best on the validation data.\n",
        "\n",
        "Early stopping is implemented by iterating through the validation data at the end of each training epoch and calculating the loss. We use the complete validation data every time and record whether or not the loss has decreased. If it has not for a number of epochs, we stop training, retrieve the best weights, and return them. When in the validation loop, we make sure not to update the model parameters. \n",
        "\n",
        "### Training Function\n",
        "\n",
        "The below function trains the network while monitoring a number of different parameters. We train with early stopping on the validation set. There are a number of parameters that I've tried to explain in the doc string. Hopefully, the comments and background make things somewhat understandable! "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qq3RgJjj2gCX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def train(model,criterion,optimizer,train_loader,valid_loader,save_file_name,\n",
        "          max_epochs_stop=3,n_epochs=20,print_every=1):\n",
        "    \"\"\"Train a PyTorch Model\n",
        "\n",
        "    Params\n",
        "    --------\n",
        "        model (PyTorch model): cnn to train\n",
        "        criterion (PyTorch loss): objective to minimize\n",
        "        optimizer (PyTorch optimizier): optimizer to compute gradients of model parameters\n",
        "        train_loader (PyTorch dataloader): training dataloader to iterate through\n",
        "        valid_loader (PyTorch dataloader): validation dataloader used for early stopping\n",
        "        save_file_name (str ending in '.pt'): file path to save the model state dict\n",
        "        max_epochs_stop (int): maximum number of epochs with no improvement in validation loss for early stopping\n",
        "        n_epochs (int): maximum number of training epochs\n",
        "        print_every (int): frequency of epochs to print training stats\n",
        "\n",
        "    Returns\n",
        "    --------\n",
        "        model (PyTorch model): trained cnn with best weights\n",
        "        history (DataFrame): history of train and validation loss and accuracy\n",
        "    \"\"\"\n",
        "\n",
        "    # Early stopping intialization\n",
        "    epochs_no_improve = 0\n",
        "    valid_loss_min = np.Inf\n",
        "\n",
        "    valid_max_acc = 0\n",
        "    history = []\n",
        "\n",
        "    # Number of epochs already trained (if using loaded in model weights)\n",
        "    try:\n",
        "        print(f'Model has been trained for: {model.epochs} epochs.\\n')\n",
        "    except:\n",
        "        model.epochs = 0\n",
        "        print(f'Starting Training from Scratch.\\n')\n",
        "\n",
        "    overall_start = timer()\n",
        "\n",
        "    # Main loop\n",
        "    for epoch in range(n_epochs):\n",
        "\n",
        "        # keep track of training and validation loss each epoch\n",
        "        train_loss = 0.0\n",
        "        valid_loss = 0.0\n",
        "\n",
        "        train_acc = 0\n",
        "        valid_acc = 0\n",
        "\n",
        "        # Set to training\n",
        "        model.train()\n",
        "        start = timer()\n",
        "\n",
        "        # Training loop\n",
        "        for ii, (data, target) in enumerate(train_loader):\n",
        "            # Tensors to gpu\n",
        "            if train_on_gpu:\n",
        "                data, target = data.cuda(), target.cuda()\n",
        "\n",
        "            # Clear gradients\n",
        "            optimizer.zero_grad()\n",
        "            # Predicted outputs are log probabilities\n",
        "            output = model(data)\n",
        "\n",
        "            # Loss and backpropagation of gradients\n",
        "            loss = criterion(output, target)\n",
        "            loss.backward()\n",
        "\n",
        "            # Update the parameters\n",
        "            optimizer.step()\n",
        "\n",
        "            # Track train loss by multiplying average loss by number of examples in batch\n",
        "            train_loss += loss.item() * data.size(0)\n",
        "\n",
        "            # Calculate accuracy by finding max log probability\n",
        "            _, pred = torch.max(output, dim=1)\n",
        "            correct_tensor = pred.eq(target.data.view_as(pred))\n",
        "            # Need to convert correct tensor from int to float to average\n",
        "            accuracy = torch.mean(correct_tensor.type(torch.FloatTensor))\n",
        "            # Multiply average accuracy times the number of examples in batch\n",
        "            train_acc += accuracy.item() * data.size(0)\n",
        "\n",
        "            # Track training progress\n",
        "            print(\n",
        "                f'Epoch: {epoch}\\t{100 * (ii + 1) / len(train_loader):.2f}% complete. {timer() - start:.2f} seconds elapsed in epoch.',\n",
        "                end='\\r')\n",
        "\n",
        "        # After training loops ends, start validation\n",
        "        else:\n",
        "            model.epochs += 1\n",
        "\n",
        "            # Don't need to keep track of gradients\n",
        "            with torch.no_grad():\n",
        "                # Set to evaluation mode\n",
        "                model.eval()\n",
        "\n",
        "                # Validation loop\n",
        "                for data, target in valid_loader:\n",
        "                    # Tensors to gpu\n",
        "                    if train_on_gpu:\n",
        "                        data, target = data.cuda(), target.cuda()\n",
        "\n",
        "                    # Forward pass\n",
        "                    output = model(data)\n",
        "\n",
        "                    # Validation loss\n",
        "                    loss = criterion(output, target)\n",
        "                    # Multiply average loss times the number of examples in batch\n",
        "                    valid_loss += loss.item() * data.size(0)\n",
        "\n",
        "                    # Calculate validation accuracy\n",
        "                    _, pred = torch.max(output, dim=1)\n",
        "                    correct_tensor = pred.eq(target.data.view_as(pred))\n",
        "                    accuracy = torch.mean(\n",
        "                        correct_tensor.type(torch.FloatTensor))\n",
        "                    # Multiply average accuracy times the number of examples\n",
        "                    valid_acc += accuracy.item() * data.size(0)\n",
        "\n",
        "                # Calculate average losses\n",
        "                train_loss = train_loss / len(train_loader.dataset)\n",
        "                valid_loss = valid_loss / len(valid_loader.dataset)\n",
        "\n",
        "                # Calculate average accuracy\n",
        "                train_acc = train_acc / len(train_loader.dataset)\n",
        "                valid_acc = valid_acc / len(valid_loader.dataset)\n",
        "\n",
        "                history.append([train_loss, valid_loss, train_acc, valid_acc])\n",
        "\n",
        "                # Print training and validation results\n",
        "                if (epoch + 1) % print_every == 0:\n",
        "                    print(\n",
        "                        f'\\nEpoch: {epoch} \\tTraining Loss: {train_loss:.4f} \\tValidation Loss: {valid_loss:.4f}'\n",
        "                    )\n",
        "                    print(\n",
        "                        f'\\t\\tTraining Accuracy: {100 * train_acc:.2f}%\\t Validation Accuracy: {100 * valid_acc:.2f}%'\n",
        "                    )\n",
        "\n",
        "                # Save the model if validation loss decreases\n",
        "                if valid_loss < valid_loss_min:\n",
        "                    # Save model\n",
        "                    torch.save(model.state_dict(), save_file_name)\n",
        "                    # Track improvement\n",
        "                    epochs_no_improve = 0\n",
        "                    valid_loss_min = valid_loss\n",
        "                    valid_best_acc = valid_acc\n",
        "                    best_epoch = epoch\n",
        "\n",
        "                # Otherwise increment count of epochs with no improvement\n",
        "                else:\n",
        "                    epochs_no_improve += 1\n",
        "                    # Trigger early stopping\n",
        "                    if epochs_no_improve >= max_epochs_stop:\n",
        "                        print(\n",
        "                            f'\\nEarly Stopping! Total epochs: {epoch}. Best epoch: {best_epoch} with loss: {valid_loss_min:.2f} and acc: {100 * valid_acc:.2f}%'\n",
        "                        )\n",
        "                        total_time = timer() - overall_start\n",
        "                        print(\n",
        "                            f'{total_time:.2f} total seconds elapsed. {total_time / (epoch+1):.2f} seconds per epoch.'\n",
        "                        )\n",
        "\n",
        "                        # Load the best state dict\n",
        "                        model.load_state_dict(torch.load(save_file_name))\n",
        "                        # Attach the optimizer\n",
        "                        model.optimizer = optimizer\n",
        "\n",
        "                        # Format history\n",
        "                        history = pd.DataFrame(\n",
        "                            history,\n",
        "                            columns=[\n",
        "                                'train_loss', 'valid_loss', 'train_acc',\n",
        "                                'valid_acc'\n",
        "                            ])\n",
        "                        return model, history\n",
        "\n",
        "    # Attach the optimizer\n",
        "    model.optimizer = optimizer\n",
        "    # Record overall time and print out stats\n",
        "    total_time = timer() - overall_start\n",
        "    print(\n",
        "        f'\\nBest epoch: {best_epoch} with loss: {valid_loss_min:.2f} and acc: {100 * valid_acc:.2f}%'\n",
        "    )\n",
        "    print(\n",
        "        f'{total_time:.2f} total seconds elapsed. {total_time / (epoch):.2f} seconds per epoch.'\n",
        "    )\n",
        "    # Format history\n",
        "    history = pd.DataFrame(\n",
        "        history,\n",
        "        columns=['train_loss', 'valid_loss', 'train_acc', 'valid_acc'])\n",
        "    return model, history"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c8VpKEKd2gCc",
        "colab_type": "code",
        "outputId": "5e4fb534-b697-4cf1-fb96-b19ea8ad8f0c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 907
        }
      },
      "source": [
        "model, history = train(\n",
        "    model,\n",
        "    criterion,\n",
        "    optimizer,\n",
        "    dataloaders['train'],\n",
        "    dataloaders['val'],\n",
        "    save_file_name=save_file_name,\n",
        "    max_epochs_stop=5,\n",
        "    n_epochs=30,\n",
        "    print_every=1)"
      ],
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model has been trained for: 1 epochs.\n",
            "\n",
            "Epoch: 0\t100.00% complete. 0.29 seconds elapsed in epoch.\n",
            "Epoch: 0 \tTraining Loss: 0.2066 \tValidation Loss: 0.4668\n",
            "\t\tTraining Accuracy: 92.59%\t Validation Accuracy: 66.67%\n",
            "Epoch: 1\t100.00% complete. 0.30 seconds elapsed in epoch.\n",
            "Epoch: 1 \tTraining Loss: 0.0550 \tValidation Loss: 0.4473\n",
            "\t\tTraining Accuracy: 100.00%\t Validation Accuracy: 83.33%\n",
            "Epoch: 2\t100.00% complete. 0.29 seconds elapsed in epoch.\n",
            "Epoch: 2 \tTraining Loss: 0.0297 \tValidation Loss: 0.3187\n",
            "\t\tTraining Accuracy: 100.00%\t Validation Accuracy: 83.33%\n",
            "Epoch: 3\t100.00% complete. 0.31 seconds elapsed in epoch.\n",
            "Epoch: 3 \tTraining Loss: 0.0689 \tValidation Loss: 0.6172\n",
            "\t\tTraining Accuracy: 96.30%\t Validation Accuracy: 83.33%\n",
            "\n",
            "Epoch: 4 \tTraining Loss: 0.1268 \tValidation Loss: 1.0140\n",
            "\t\tTraining Accuracy: 96.30%\t Validation Accuracy: 66.67%\n",
            "Epoch: 5\t100.00% complete. 0.22 seconds elapsed in epoch.\n",
            "Epoch: 5 \tTraining Loss: 0.2649 \tValidation Loss: 0.4961\n",
            "\t\tTraining Accuracy: 88.89%\t Validation Accuracy: 83.33%\n",
            "Epoch: 6\t100.00% complete. 0.22 seconds elapsed in epoch.\n",
            "Epoch: 6 \tTraining Loss: 0.0516 \tValidation Loss: 0.6269\n",
            "\t\tTraining Accuracy: 96.30%\t Validation Accuracy: 66.67%\n",
            "\n",
            "Epoch: 7 \tTraining Loss: 0.3902 \tValidation Loss: 0.1200\n",
            "\t\tTraining Accuracy: 96.30%\t Validation Accuracy: 100.00%\n",
            "Epoch: 8\t100.00% complete. 0.28 seconds elapsed in epoch.\n",
            "Epoch: 8 \tTraining Loss: 0.1043 \tValidation Loss: 0.1379\n",
            "\t\tTraining Accuracy: 96.30%\t Validation Accuracy: 100.00%\n",
            "Epoch: 9\t100.00% complete. 0.21 seconds elapsed in epoch.\n",
            "Epoch: 9 \tTraining Loss: 0.2114 \tValidation Loss: 0.4021\n",
            "\t\tTraining Accuracy: 92.59%\t Validation Accuracy: 66.67%\n",
            "\n",
            "Epoch: 10 \tTraining Loss: 0.0025 \tValidation Loss: 0.0872\n",
            "\t\tTraining Accuracy: 100.00%\t Validation Accuracy: 100.00%\n",
            "Epoch: 11\t100.00% complete. 0.29 seconds elapsed in epoch.\n",
            "Epoch: 11 \tTraining Loss: 0.0688 \tValidation Loss: 2.6903\n",
            "\t\tTraining Accuracy: 96.30%\t Validation Accuracy: 50.00%\n",
            "Epoch: 12\t100.00% complete. 0.21 seconds elapsed in epoch.\n",
            "Epoch: 12 \tTraining Loss: 0.0092 \tValidation Loss: 2.0230\n",
            "\t\tTraining Accuracy: 100.00%\t Validation Accuracy: 50.00%\n",
            "Epoch: 13\t100.00% complete. 0.21 seconds elapsed in epoch.\n",
            "Epoch: 13 \tTraining Loss: 0.0015 \tValidation Loss: 1.2501\n",
            "\t\tTraining Accuracy: 100.00%\t Validation Accuracy: 66.67%\n",
            "Epoch: 14\t100.00% complete. 0.21 seconds elapsed in epoch.\n",
            "Epoch: 14 \tTraining Loss: 0.0010 \tValidation Loss: 0.8718\n",
            "\t\tTraining Accuracy: 100.00%\t Validation Accuracy: 66.67%\n",
            "Epoch: 15\t100.00% complete. 0.21 seconds elapsed in epoch.\n",
            "Epoch: 15 \tTraining Loss: 0.0001 \tValidation Loss: 0.7391\n",
            "\t\tTraining Accuracy: 100.00%\t Validation Accuracy: 66.67%\n",
            "\n",
            "Early Stopping! Total epochs: 15. Best epoch: 10 with loss: 0.09 and acc: 66.67%\n",
            "22.90 total seconds elapsed. 1.43 seconds per epoch.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cDgmbBr02gCh",
        "colab_type": "text"
      },
      "source": [
        "# Training Results\n",
        "\n",
        "We can inspect the training progress by looking at the `history`. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6lZ2a9Ax2gCk",
        "colab_type": "code",
        "outputId": "bf467132-86aa-4986-a110-70d0208c6afd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 421
        }
      },
      "source": [
        "plt.figure(figsize=(8, 6))\n",
        "for c in ['train_loss', 'valid_loss']:\n",
        "    plt.plot(\n",
        "        history[c], label=c)\n",
        "plt.legend()\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Average Negative Log Likelihood')\n",
        "plt.title('Training and Validation Losses')"
      ],
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Text(0.5, 1.0, 'Training and Validation Losses')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 45
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAfUAAAGDCAYAAAAyM4nNAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAABJRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 576x432 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QfjDW2sd2gCn",
        "colab_type": "text"
      },
      "source": [
        "As expected, the training loss decreases continually with epochs. At a certain point however, the validation loss stops decreasing. There is not a massive amount of overfitting, likely because we were using Dropout. With the divergence in losses, there is likely not much more to gain from further training."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zhmq6Eu72gCo",
        "colab_type": "code",
        "outputId": "b581118c-e89d-45fa-c33c-a1e689577717",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 421
        }
      },
      "source": [
        "plt.figure(figsize=(8, 6))\n",
        "for c in ['train_acc', 'valid_acc']:\n",
        "    plt.plot(\n",
        "        100 * history[c], label=c)\n",
        "plt.legend()\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Average Accuracy')\n",
        "plt.title('Training and Validation Accuracy')"
      ],
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Text(0.5, 1.0, 'Training and Validation Accuracy')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 46
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAfgAAAGDCAYAAADHzQJ9AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 576x432 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HVgvaLV72gCu",
        "colab_type": "text"
      },
      "source": [
        "As with the losses, the training accuracy increases (nearly to perfect) while the validation accuracy plateaus. The model is able to achieve above 80% accuracy right away, an indication that the convolution weights learned on Imagenet were able to easily transfer to our dataset."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zCtgG5da2gCu",
        "colab_type": "text"
      },
      "source": [
        "# Saving Model\n",
        "\n",
        "The `train` function saves the best model `state_dict()` which are the weights of the model. To save more information about the model, we use the below function. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l00_LAZi2gCv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def save_checkpoint(model, path):\n",
        "    \"\"\"Save a PyTorch model checkpoint\n",
        "\n",
        "    Params\n",
        "    --------\n",
        "        model (PyTorch model): model to save\n",
        "        path (str): location to save model. Must start with `model_name-` and end in '.pth'\n",
        "\n",
        "    Returns\n",
        "    --------\n",
        "        None, save the `model` to `path`\n",
        "\n",
        "    \"\"\"\n",
        "\n",
        "    model_name = path.split('-')[0]\n",
        "    assert (model_name in ['vgg16', 'resnet50'\n",
        "                           ]), \"Path must have the correct model name\"\n",
        "\n",
        "    # Basic details\n",
        "    checkpoint = {\n",
        "        'class_to_idx': model.class_to_idx,\n",
        "        'idx_to_class': model.idx_to_class,\n",
        "        'epochs': model.epochs,\n",
        "    }\n",
        "\n",
        "    # Extract the final classifier and the state dictionary\n",
        "    if model_name == 'vgg16':\n",
        "        # Check to see if model was parallelized\n",
        "        if multi_gpu:\n",
        "            checkpoint['classifier'] = model.module.classifier\n",
        "            checkpoint['state_dict'] = model.module.state_dict()\n",
        "        else:\n",
        "            checkpoint['classifier'] = model.classifier\n",
        "            checkpoint['state_dict'] = model.state_dict()\n",
        "\n",
        "    elif model_name == 'resnet50':\n",
        "        if multi_gpu:\n",
        "            checkpoint['fc'] = model.module.fc\n",
        "            checkpoint['state_dict'] = model.module.state_dict()\n",
        "        else:\n",
        "            checkpoint['fc'] = model.fc\n",
        "            checkpoint['state_dict'] = model.state_dict()\n",
        "\n",
        "    # Add the optimizer\n",
        "    checkpoint['optimizer'] = model.optimizer\n",
        "    checkpoint['optimizer_state_dict'] = model.optimizer.state_dict()\n",
        "\n",
        "    # Save the data to the path\n",
        "    torch.save(checkpoint, path)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HDaocFe-2gCw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "save_checkpoint(model, path=checkpoint_path)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GoUKTQCs2gCy",
        "colab_type": "text"
      },
      "source": [
        "## Load in a Checkpoint\n",
        "\n",
        "Now we need to write the function to load in the checkpoint. This just takes in a `path` and returns a model from a saved checkpoint."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1YUFhXs-2gCy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def load_checkpoint(path):\n",
        "    \"\"\"Load a PyTorch model checkpoint\n",
        "\n",
        "    Params\n",
        "    --------\n",
        "        path (str): saved model checkpoint. Must start with `model_name-` and end in '.pth'\n",
        "\n",
        "    Returns\n",
        "    --------\n",
        "        None, save the `model` to `path`\n",
        "\n",
        "    \"\"\"\n",
        "\n",
        "    # Get the model name\n",
        "    model_name = path.split('-')[0]\n",
        "    assert (model_name in ['vgg16', 'resnet50'\n",
        "                           ]), \"Path must have the correct model name\"\n",
        "\n",
        "    # Load in checkpoint\n",
        "    checkpoint = torch.load(path)\n",
        "\n",
        "    if model_name == 'vgg16':\n",
        "        model = models.vgg16(pretrained=True)\n",
        "        # Make sure to set parameters as not trainable\n",
        "        for param in model.parameters():\n",
        "            param.requires_grad = False\n",
        "        model.classifier = checkpoint['classifier']\n",
        "\n",
        "\n",
        "    # Load in the state dict\n",
        "    model.load_state_dict(checkpoint['state_dict'])\n",
        "\n",
        "    total_params = sum(p.numel() for p in model.parameters())\n",
        "    print(f'{total_params:,} total parameters.')\n",
        "    total_trainable_params = sum(\n",
        "        p.numel() for p in model.parameters() if p.requires_grad)\n",
        "    print(f'{total_trainable_params:,} total gradient parameters.')\n",
        "\n",
        "    # Move to gpu\n",
        "    if train_on_gpu:\n",
        "        model = model.to('cuda')\n",
        "\n",
        "    # Model basics\n",
        "    model.class_to_idx = checkpoint['class_to_idx']\n",
        "    model.idx_to_class = checkpoint['idx_to_class']\n",
        "    model.epochs = checkpoint['epochs']\n",
        "\n",
        "    # Optimizer\n",
        "    optimizer = checkpoint['optimizer']\n",
        "    optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
        "\n",
        "    return model, optimizer"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cTBemifF2gC2",
        "colab_type": "code",
        "outputId": "5085ccfb-28be-49a0-a264-fffdb46aea5d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 991
        }
      },
      "source": [
        "model, optimizer = load_checkpoint(path=checkpoint_path)\n",
        "\n",
        "if multi_gpu:\n",
        "    summary(model.module, input_size=(3, 224, 224), batch_size=batch_size)\n",
        "else:\n",
        "    summary(model, input_size=(3, 224, 224), batch_size=batch_size)"
      ],
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "135,309,890 total parameters.\n",
            "1,049,346 total gradient parameters.\n",
            "----------------------------------------------------------------\n",
            "        Layer (type)               Output Shape         Param #\n",
            "================================================================\n",
            "            Conv2d-1          [1, 64, 224, 224]           1,792\n",
            "              ReLU-2          [1, 64, 224, 224]               0\n",
            "            Conv2d-3          [1, 64, 224, 224]          36,928\n",
            "              ReLU-4          [1, 64, 224, 224]               0\n",
            "         MaxPool2d-5          [1, 64, 112, 112]               0\n",
            "            Conv2d-6         [1, 128, 112, 112]          73,856\n",
            "              ReLU-7         [1, 128, 112, 112]               0\n",
            "            Conv2d-8         [1, 128, 112, 112]         147,584\n",
            "              ReLU-9         [1, 128, 112, 112]               0\n",
            "        MaxPool2d-10           [1, 128, 56, 56]               0\n",
            "           Conv2d-11           [1, 256, 56, 56]         295,168\n",
            "             ReLU-12           [1, 256, 56, 56]               0\n",
            "           Conv2d-13           [1, 256, 56, 56]         590,080\n",
            "             ReLU-14           [1, 256, 56, 56]               0\n",
            "           Conv2d-15           [1, 256, 56, 56]         590,080\n",
            "             ReLU-16           [1, 256, 56, 56]               0\n",
            "        MaxPool2d-17           [1, 256, 28, 28]               0\n",
            "           Conv2d-18           [1, 512, 28, 28]       1,180,160\n",
            "             ReLU-19           [1, 512, 28, 28]               0\n",
            "           Conv2d-20           [1, 512, 28, 28]       2,359,808\n",
            "             ReLU-21           [1, 512, 28, 28]               0\n",
            "           Conv2d-22           [1, 512, 28, 28]       2,359,808\n",
            "             ReLU-23           [1, 512, 28, 28]               0\n",
            "        MaxPool2d-24           [1, 512, 14, 14]               0\n",
            "           Conv2d-25           [1, 512, 14, 14]       2,359,808\n",
            "             ReLU-26           [1, 512, 14, 14]               0\n",
            "           Conv2d-27           [1, 512, 14, 14]       2,359,808\n",
            "             ReLU-28           [1, 512, 14, 14]               0\n",
            "           Conv2d-29           [1, 512, 14, 14]       2,359,808\n",
            "             ReLU-30           [1, 512, 14, 14]               0\n",
            "        MaxPool2d-31             [1, 512, 7, 7]               0\n",
            "AdaptiveAvgPool2d-32             [1, 512, 7, 7]               0\n",
            "           Linear-33                  [1, 4096]     102,764,544\n",
            "             ReLU-34                  [1, 4096]               0\n",
            "          Dropout-35                  [1, 4096]               0\n",
            "           Linear-36                  [1, 4096]      16,781,312\n",
            "             ReLU-37                  [1, 4096]               0\n",
            "          Dropout-38                  [1, 4096]               0\n",
            "           Linear-39                   [1, 256]       1,048,832\n",
            "             ReLU-40                   [1, 256]               0\n",
            "          Dropout-41                   [1, 256]               0\n",
            "           Linear-42                     [1, 2]             514\n",
            "       LogSoftmax-43                     [1, 2]               0\n",
            "================================================================\n",
            "Total params: 135,309,890\n",
            "Trainable params: 1,049,346\n",
            "Non-trainable params: 134,260,544\n",
            "----------------------------------------------------------------\n",
            "Input size (MB): 0.57\n",
            "Forward/backward pass size (MB): 218.78\n",
            "Params size (MB): 516.17\n",
            "Estimated Total Size (MB): 735.52\n",
            "----------------------------------------------------------------\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AnONZYgd2gC4",
        "colab_type": "text"
      },
      "source": [
        "We can now use these two functions to save and load in a complete model. We can use this model to continue training. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bZS9Gmqo2gC5",
        "colab_type": "code",
        "outputId": "80f4fce5-eb45-424e-ad1b-350a9c695bec",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 706
        }
      },
      "source": [
        "model, history = train(model,criterion,optimizer,dataloaders['train'],dataloaders['val'],\n",
        "                       save_file_name=save_file_name,max_epochs_stop=5,n_epochs=30,print_every=1)"
      ],
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model has been trained for: 24 epochs.\n",
            "\n",
            "Epoch: 0\t100.00% complete. 0.31 seconds elapsed in epoch.\n",
            "Epoch: 0 \tTraining Loss: 0.0004 \tValidation Loss: 0.8098\n",
            "\t\tTraining Accuracy: 100.00%\t Validation Accuracy: 50.00%\n",
            "Epoch: 1\t100.00% complete. 0.30 seconds elapsed in epoch.\n",
            "Epoch: 1 \tTraining Loss: 0.0055 \tValidation Loss: 0.5235\n",
            "\t\tTraining Accuracy: 100.00%\t Validation Accuracy: 66.67%\n",
            "Epoch: 2\t100.00% complete. 0.31 seconds elapsed in epoch.\n",
            "Epoch: 2 \tTraining Loss: 0.0000 \tValidation Loss: 0.4898\n",
            "\t\tTraining Accuracy: 100.00%\t Validation Accuracy: 66.67%\n",
            "Epoch: 3\t100.00% complete. 0.31 seconds elapsed in epoch.\n",
            "Epoch: 3 \tTraining Loss: 0.2212 \tValidation Loss: 2.2144\n",
            "\t\tTraining Accuracy: 92.59%\t Validation Accuracy: 50.00%\n",
            "\n",
            "Epoch: 4 \tTraining Loss: 0.0002 \tValidation Loss: 0.8495\n",
            "\t\tTraining Accuracy: 100.00%\t Validation Accuracy: 66.67%\n",
            "Epoch: 5\t100.00% complete. 0.26 seconds elapsed in epoch.\n",
            "Epoch: 5 \tTraining Loss: 0.1723 \tValidation Loss: 2.5555\n",
            "\t\tTraining Accuracy: 96.30%\t Validation Accuracy: 50.00%\n",
            "Epoch: 6\t100.00% complete. 0.22 seconds elapsed in epoch.\n",
            "Epoch: 6 \tTraining Loss: 0.0414 \tValidation Loss: 0.4388\n",
            "\t\tTraining Accuracy: 96.30%\t Validation Accuracy: 83.33%\n",
            "Epoch: 7\t100.00% complete. 0.30 seconds elapsed in epoch.\n",
            "Epoch: 7 \tTraining Loss: 0.0006 \tValidation Loss: 0.6973\n",
            "\t\tTraining Accuracy: 100.00%\t Validation Accuracy: 83.33%\n",
            "\n",
            "Epoch: 8 \tTraining Loss: 0.0000 \tValidation Loss: 0.7746\n",
            "\t\tTraining Accuracy: 100.00%\t Validation Accuracy: 83.33%\n",
            "Epoch: 9\t100.00% complete. 0.22 seconds elapsed in epoch.\n",
            "Epoch: 9 \tTraining Loss: 0.0000 \tValidation Loss: 0.7076\n",
            "\t\tTraining Accuracy: 100.00%\t Validation Accuracy: 83.33%\n",
            "Epoch: 10\t100.00% complete. 0.22 seconds elapsed in epoch.\n",
            "Epoch: 10 \tTraining Loss: 0.0019 \tValidation Loss: 0.4756\n",
            "\t\tTraining Accuracy: 100.00%\t Validation Accuracy: 66.67%\n",
            "Epoch: 11\t100.00% complete. 0.21 seconds elapsed in epoch.\n",
            "Epoch: 11 \tTraining Loss: 0.0001 \tValidation Loss: 0.4789\n",
            "\t\tTraining Accuracy: 100.00%\t Validation Accuracy: 66.67%\n",
            "\n",
            "Early Stopping! Total epochs: 11. Best epoch: 6 with loss: 0.44 and acc: 66.67%\n",
            "15.89 total seconds elapsed. 1.32 seconds per epoch.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vqkeXpF92gC8",
        "colab_type": "text"
      },
      "source": [
        "Further training is unlikely to improve the validation results"
      ]
    }
  ]
}